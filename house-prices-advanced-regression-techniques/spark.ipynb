{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- MSSubClass: integer (nullable = true)\n",
      " |-- MSZoning: string (nullable = true)\n",
      " |-- LotFrontage: string (nullable = true)\n",
      " |-- LotArea: integer (nullable = true)\n",
      " |-- Street: string (nullable = true)\n",
      " |-- Alley: string (nullable = true)\n",
      " |-- LotShape: string (nullable = true)\n",
      " |-- LandContour: string (nullable = true)\n",
      " |-- Utilities: string (nullable = true)\n",
      " |-- LotConfig: string (nullable = true)\n",
      " |-- LandSlope: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Condition1: string (nullable = true)\n",
      " |-- Condition2: string (nullable = true)\n",
      " |-- BldgType: string (nullable = true)\n",
      " |-- HouseStyle: string (nullable = true)\n",
      " |-- OverallQual: integer (nullable = true)\n",
      " |-- OverallCond: integer (nullable = true)\n",
      " |-- YearBuilt: integer (nullable = true)\n",
      " |-- YearRemodAdd: integer (nullable = true)\n",
      " |-- RoofStyle: string (nullable = true)\n",
      " |-- RoofMatl: string (nullable = true)\n",
      " |-- Exterior1st: string (nullable = true)\n",
      " |-- Exterior2nd: string (nullable = true)\n",
      " |-- MasVnrType: string (nullable = true)\n",
      " |-- MasVnrArea: string (nullable = true)\n",
      " |-- ExterQual: string (nullable = true)\n",
      " |-- ExterCond: string (nullable = true)\n",
      " |-- Foundation: string (nullable = true)\n",
      " |-- BsmtQual: string (nullable = true)\n",
      " |-- BsmtCond: string (nullable = true)\n",
      " |-- BsmtExposure: string (nullable = true)\n",
      " |-- BsmtFinType1: string (nullable = true)\n",
      " |-- BsmtFinSF1: integer (nullable = true)\n",
      " |-- BsmtFinType2: string (nullable = true)\n",
      " |-- BsmtFinSF2: integer (nullable = true)\n",
      " |-- BsmtUnfSF: integer (nullable = true)\n",
      " |-- TotalBsmtSF: integer (nullable = true)\n",
      " |-- Heating: string (nullable = true)\n",
      " |-- HeatingQC: string (nullable = true)\n",
      " |-- CentralAir: string (nullable = true)\n",
      " |-- Electrical: string (nullable = true)\n",
      " |-- 1stFlrSF: integer (nullable = true)\n",
      " |-- 2ndFlrSF: integer (nullable = true)\n",
      " |-- LowQualFinSF: integer (nullable = true)\n",
      " |-- GrLivArea: integer (nullable = true)\n",
      " |-- BsmtFullBath: integer (nullable = true)\n",
      " |-- BsmtHalfBath: integer (nullable = true)\n",
      " |-- FullBath: integer (nullable = true)\n",
      " |-- HalfBath: integer (nullable = true)\n",
      " |-- BedroomAbvGr: integer (nullable = true)\n",
      " |-- KitchenAbvGr: integer (nullable = true)\n",
      " |-- KitchenQual: string (nullable = true)\n",
      " |-- TotRmsAbvGrd: integer (nullable = true)\n",
      " |-- Functional: string (nullable = true)\n",
      " |-- Fireplaces: integer (nullable = true)\n",
      " |-- FireplaceQu: string (nullable = true)\n",
      " |-- GarageType: string (nullable = true)\n",
      " |-- GarageYrBlt: string (nullable = true)\n",
      " |-- GarageFinish: string (nullable = true)\n",
      " |-- GarageCars: integer (nullable = true)\n",
      " |-- GarageArea: integer (nullable = true)\n",
      " |-- GarageQual: string (nullable = true)\n",
      " |-- GarageCond: string (nullable = true)\n",
      " |-- PavedDrive: string (nullable = true)\n",
      " |-- WoodDeckSF: integer (nullable = true)\n",
      " |-- OpenPorchSF: integer (nullable = true)\n",
      " |-- EnclosedPorch: integer (nullable = true)\n",
      " |-- 3SsnPorch: integer (nullable = true)\n",
      " |-- ScreenPorch: integer (nullable = true)\n",
      " |-- PoolArea: integer (nullable = true)\n",
      " |-- PoolQC: string (nullable = true)\n",
      " |-- Fence: string (nullable = true)\n",
      " |-- MiscFeature: string (nullable = true)\n",
      " |-- MiscVal: integer (nullable = true)\n",
      " |-- MoSold: integer (nullable = true)\n",
      " |-- YrSold: integer (nullable = true)\n",
      " |-- SaleType: string (nullable = true)\n",
      " |-- SaleCondition: string (nullable = true)\n",
      " |-- SalePrice: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()\n",
    "\n",
    "train = spark.read.csv('./train.csv', header='true', inferSchema='true')\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will explore the label columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         SalePrice|\n",
      "+-------+------------------+\n",
      "|  count|              1460|\n",
      "|   mean|180921.19589041095|\n",
      "| stddev| 79442.50288288663|\n",
      "|    min|             34900|\n",
      "|    max|            755000|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------------------+\n",
      "|skewness(SalePrice)|\n",
      "+-------------------+\n",
      "| 1.8809407460340333|\n",
      "+-------------------+\n",
      "\n",
      "+-------------------+\n",
      "|kurtosis(SalePrice)|\n",
      "+-------------------+\n",
      "|  6.509812011089432|\n",
      "+-------------------+\n",
      "\n",
      "[127000.0, 157000.0, 205000.0]\n",
      "+--------------------+\n",
      "| SalePrice_freqItems|\n",
      "+--------------------+\n",
      "|[210000, 142125, ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import kurtosis, skewness\n",
    "\n",
    "train.select('SalePrice').describe().show()\n",
    "train.agg(skewness('SalePrice')).show()\n",
    "train.agg(kurtosis('SalePrice')).show()\n",
    "# Last parameter is error tolerance\n",
    "print(train.approxQuantile('SalePrice', [0.25, 0.50, 0.75], 0.05))\n",
    "train.stat.freqItems(['SalePrice'], 0.1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMissingValuesCols(df):\n",
    "    numRows = df.count()\n",
    "    nullCols = []\n",
    "    for column in df.columns:\n",
    "        c = df.filter(col(column).isNotNull()).count()\n",
    "        if c != numRows:\n",
    "            nullCols.append(c)\n",
    "    return nullCols\n",
    "\n",
    "missingCols = findMissingValuesCols(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f97023ec668>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "corrCols = [dtype[0] for dtype in train.dtypes if dtype[1] == 'int']\n",
    "corrCols.remove('Id')\n",
    "\n",
    "corrAssembler = VectorAssembler(inputCols=corrCols, outputCol='corrFeatures')\n",
    "corrTrain = corrAssembler.transform(train)\n",
    "corrMat = Correlation.corr(corrTrain, 'corrFeatures').head()\n",
    "\n",
    "pdf = corrMat[0].toArray()\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "sns.heatmap(pdf, \n",
    "            xticklabels=corrCols,\n",
    "            yticklabels=corrCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def autoIndexer(df, maxDistinct):\n",
    "    stringTypes = [dtype[0] for dtype in train.dtypes if dtype[1] == 'string']\n",
    "    numRows = train.count()\n",
    "    indexed = df\n",
    "    indexedCols = []\n",
    "    for column in stringTypes:\n",
    "        distinctCount = train.select(column).distinct().count()\n",
    "        if distinctCount < maxDistinct:\n",
    "            indexedCol = 'Indexed' + column\n",
    "            indexedCols.append(indexedCol)\n",
    "            indexer = StringIndexer(inputCol=column, outputCol=indexedCol)\n",
    "            indexed = indexer.fit(indexed).transform(indexed)\n",
    "            indexed = indexed.drop(column)\n",
    "    return indexedCols, indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "oneHotEncodedColumns = ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n",
    "                       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n",
    "                       'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'Electrical', 'GarageType',\n",
    "                       'MiscFeature', ]\n",
    "indexedOutputColumns = ['Indexed'+column for column in oneHotEncodedColumns]\n",
    "oneHotEncodedOutputColumns = ['Encoded'+column for column in indexedOutputColumns]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol='Indexed'+column) for column in oneHotEncodedColumns]\n",
    "\n",
    "oneHotEncoder = OneHotEncoderEstimator(inputCols=indexedOutputColumns, outputCols=oneHotEncodedOutputColumns)\n",
    "# pipeline = Pipeline(stages=[*indexers, estimator])\n",
    "# model = pipeline.fit(train)\n",
    "# cleaned = model.transform(train)\n",
    "# droppedColumns = [*oneHotEncodedColumns, *indexedOutputColumns]\n",
    "# cleaned = cleaned.drop(*droppedColumns)\n",
    "# cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false \n",
    "\n",
    "cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'\n",
    "       'HeatingQC', 'KitchenQual', 'Functional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='ChiSqSelector_4369b6e8a3cb4650e264', name='percentile', doc='Percentile of features that selector will select, ordered by ascending p-value.')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [dtype[0] for dtype in train.dtypes if dtype[1]!='string']\n",
    "cols.remove('SalePrice')\n",
    "\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "chiSqAssembler = VectorAssembler(inputCols=cols, outputCol='features')\n",
    "cleaned = chiSqAssembler.transform(train)\n",
    "chiSqSelector = ChiSqSelector(numTopFeatures=10, featuresCol='features', outputCol='selectedFeatures', labelCol='SalePrice')\n",
    "cleaned = chiSqSelector.fit(cleaned).transform(cleaned)\n",
    "chiSqSelector.percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "randomForest = RandomForestRegressor(featuresCol='selectedFeatures', labelCol='SalePrice', seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8625773603318725"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# grid = ParamGridBuilder().addGrid(randomForest.numTrees, [10, 15, 20]).build()\n",
    "\n",
    "pipeline = Pipeline(stages=[*indexers, oneHotEncoder, chiSqAssembler, chiSqSelector, randomForest])\n",
    "\n",
    "# crossval = CrossValidator(estimator=pipeline, \n",
    "#                           estimatorParamMaps=grid, \n",
    "#                           evaluator=RegressionEvaluator(labelCol=\"SalePrice\"), \n",
    "#                           numFolds=10)\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "df_r = model.transform(train)\n",
    "evaluator=RegressionEvaluator(labelCol=\"SalePrice\")\n",
    "evaluator.evaluate(df_r,  {evaluator.metricName: \"r2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  10\n",
      "Number of trees:  20\n",
      "Feature Importances:  (10,[0,1,2,3,4,5,6,7,8,9],[0.0192426156999,0.459307192737,0.00972816499007,0.00905705388025,0.105944178937,0.06505743252,0.0317621143464,0.229945465731,0.0426214544914,0.0273343266662])\n"
     ]
    }
   ],
   "source": [
    "rfModel = model.stages[-1]\n",
    "\n",
    "print('Number of features: ', rfModel.numFeatures)\n",
    "print('Number of trees: ', rfModel.getNumTrees)\n",
    "print('Feature Importances: ', rfModel.featureImportances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BsmtFinSF1: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- BsmtFinSF1: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Data type StringType of column BsmtUnfSF is not supported.\\nData type StringType of column TotalBsmtSF is not supported.\\nData type StringType of column BsmtFullBath is not supported.\\nData type StringType of column BsmtHalfBath is not supported.\\nData type StringType of column GarageCars is not supported.\\nData type StringType of column GarageArea is not supported.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o381.transform.\n: java.lang.IllegalArgumentException: Data type StringType of column BsmtUnfSF is not supported.\nData type StringType of column TotalBsmtSF is not supported.\nData type StringType of column BsmtFullBath is not supported.\nData type StringType of column BsmtHalfBath is not supported.\nData type StringType of column GarageCars is not supported.\nData type StringType of column GarageArea is not supported.\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:124)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:54)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-be6a394acdbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BsmtFinSF1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Data type StringType of column BsmtUnfSF is not supported.\\nData type StringType of column TotalBsmtSF is not supported.\\nData type StringType of column BsmtFullBath is not supported.\\nData type StringType of column BsmtHalfBath is not supported.\\nData type StringType of column GarageCars is not supported.\\nData type StringType of column GarageArea is not supported.'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "test = spark.read.csv('./test.csv', header='true', inferSchema='true')\n",
    "test = test.withColumn('BsmtFinSF1', test['BsmtFinSF1'].cast(IntegerType()))\n",
    "test = test.withColumn('BsmtFinSF2', test['BsmtFinSF2'].cast(IntegerType()))\n",
    "train.select('BsmtFinSF1').printSchema()\n",
    "test.select('BsmtFinSF1').printSchema()\n",
    "\n",
    "predictions = model.transform(test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = indexed.columns[:]\n",
    "cols.remove('SalePrice')\n",
    "cols.remove('Id')\n",
    "print(cols)\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol='features')\n",
    "assembled = assembler.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import ChiSquareTest, Correlation\n",
    "\n",
    "r = ChiSquareTest.test(assembled, 'features', 'SalePrice')\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))\n",
    "\n",
    "r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = Correlation.corr(assembled, 'features')\n",
    "r1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.crosstab('OverallQual', 'OverallCond').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.corr('OverallCond', 'OverallQual')\n",
    "# train.dtypes\n",
    "train.select('OverallCond').groupby('OverallCond')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
