{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"titanic\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.csv('./train.csv', header=\"true\", inferSchema=\"true\")\n",
    "test = spark.read.csv('./test.csv', header=\"true\", inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age', 'Cabin', 'Embarked']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Find columns with missing values\n",
    "def findNullColumns(df):\n",
    "    nullCols = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        if df.filter(col(k).isNotNull()).count() != numRows:\n",
    "            nullCols.append(k)\n",
    "    return nullCols\n",
    "\n",
    "findNullColumns(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19865319865319866"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find percentage of data missing values\n",
    "\n",
    "train.filter(train.Age.isNull()).count() / train.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.filter(train.Age.isNotNull())\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.filter(train.Embarked.isNull()).count() / train.count()\n",
    "train = train.filter(train.Embarked.isNotNull())\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.filter(train.Cabin.isNull()).count() / train.count()\n",
    "train = train.drop('Cabin')\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop('Cabin')\n",
    "train = train.drop('Embarked')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- BucketedAge: double (nullable = true)\n",
      " |-- BucketedFare: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "ageSplits = [0, 16, 32, 48, 64, 200]\n",
    "ageBucketizer = Bucketizer(splits=ageSplits, inputCol='Age', outputCol='BucketedAge')\n",
    "fareSplits = [-float('inf'), 7.91, 14.454, 31, float('inf')]\n",
    "fareBucketizer = Bucketizer(splits=fareSplits, inputCol='Fare', outputCol='BucketedFare')\n",
    "\n",
    "train_cleaned = ageBucketizer.transform(train).drop('Age')\n",
    "train_cleaned = fareBucketizer.transform(train_cleaned).drop('Fare')\n",
    "train_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+-----+-----+----------------+-----------+------------+----------+\n",
      "|PassengerId|Survived|Pclass|                Name|SibSp|Parch|          Ticket|BucketedAge|BucketedFare|IndexedSex|\n",
      "+-----------+--------+------+--------------------+-----+-----+----------------+-----------+------------+----------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|    1|    0|       A/5 21171|        1.0|         0.0|       0.0|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|    1|    0|        PC 17599|        2.0|         3.0|       1.0|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|    0|    0|STON/O2. 3101282|        1.0|         1.0|       1.0|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|    1|    0|          113803|        2.0|         3.0|       1.0|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|    0|    0|          373450|        2.0|         1.0|       0.0|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|    0|    0|           17463|        3.0|         3.0|       0.0|\n",
      "|          8|       0|     3|Palsson, Master. ...|    3|    1|          349909|        0.0|         2.0|       0.0|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|    0|    2|          347742|        1.0|         1.0|       1.0|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|    1|    0|          237736|        0.0|         2.0|       1.0|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|    1|    1|         PP 9549|        0.0|         2.0|       1.0|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|    0|    0|          113783|        3.0|         2.0|       1.0|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|    0|    0|       A/5. 2151|        1.0|         1.0|       0.0|\n",
      "|         14|       0|     3|Andersson, Mr. An...|    1|    5|          347082|        2.0|         3.0|       0.0|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|    0|    0|          350406|        0.0|         0.0|       1.0|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|    0|    0|          248706|        3.0|         2.0|       1.0|\n",
      "|         17|       0|     3|Rice, Master. Eugene|    4|    1|          382652|        0.0|         2.0|       0.0|\n",
      "|         19|       0|     3|Vander Planke, Mr...|    1|    0|          345763|        1.0|         2.0|       1.0|\n",
      "|         21|       0|     2|Fynney, Mr. Joseph J|    0|    0|          239865|        2.0|         2.0|       0.0|\n",
      "|         22|       1|     2|Beesley, Mr. Lawr...|    0|    0|          248698|        2.0|         1.0|       0.0|\n",
      "|         23|       1|     3|\"McGowan, Miss. A...|    0|    0|          330923|        0.0|         1.0|       1.0|\n",
      "+-----------+--------+------+--------------------+-----+-----+----------------+-----------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "\n",
    "sexFeatureIndexer = StringIndexer(inputCol='Sex', outputCol='IndexedSex').fit(train_cleaned)\n",
    "train_cleaned = sexFeatureIndexer.transform(train_cleaned).drop('Sex')\n",
    "\n",
    "train_cleaned.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+-----+-----------+------------+----------+\n",
      "|Survived|Pclass|SibSp|Parch|BucketedAge|BucketedFare|IndexedSex|\n",
      "+--------+------+-----+-----+-----------+------------+----------+\n",
      "|       0|     3|    1|    0|        1.0|         0.0|       0.0|\n",
      "|       1|     1|    1|    0|        2.0|         3.0|       1.0|\n",
      "|       1|     3|    0|    0|        1.0|         1.0|       1.0|\n",
      "|       1|     1|    1|    0|        2.0|         3.0|       1.0|\n",
      "|       0|     3|    0|    0|        2.0|         1.0|       0.0|\n",
      "|       0|     1|    0|    0|        3.0|         3.0|       0.0|\n",
      "|       0|     3|    3|    1|        0.0|         2.0|       0.0|\n",
      "|       1|     3|    0|    2|        1.0|         1.0|       1.0|\n",
      "|       1|     2|    1|    0|        0.0|         2.0|       1.0|\n",
      "|       1|     3|    1|    1|        0.0|         2.0|       1.0|\n",
      "|       1|     1|    0|    0|        3.0|         2.0|       1.0|\n",
      "|       0|     3|    0|    0|        1.0|         1.0|       0.0|\n",
      "|       0|     3|    1|    5|        2.0|         3.0|       0.0|\n",
      "|       0|     3|    0|    0|        0.0|         0.0|       1.0|\n",
      "|       1|     2|    0|    0|        3.0|         2.0|       1.0|\n",
      "|       0|     3|    4|    1|        0.0|         2.0|       0.0|\n",
      "|       0|     3|    1|    0|        1.0|         2.0|       1.0|\n",
      "|       0|     2|    0|    0|        2.0|         2.0|       0.0|\n",
      "|       1|     2|    0|    0|        2.0|         1.0|       0.0|\n",
      "|       1|     3|    0|    0|        0.0|         1.0|       1.0|\n",
      "+--------+------+-----+-----+-----------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_cleaned = train_cleaned.drop('Ticket')\n",
    "train_cleaned = train_cleaned.drop('PassengerId')\n",
    "train_cleaned = train_cleaned.drop('Name')\n",
    "train_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|Survived|            features|\n",
      "+--------+--------------------+\n",
      "|       0|[3.0,1.0,0.0,1.0,...|\n",
      "|       1|[1.0,1.0,0.0,2.0,...|\n",
      "|       1|[3.0,0.0,0.0,1.0,...|\n",
      "|       1|[1.0,1.0,0.0,2.0,...|\n",
      "|       0|[3.0,0.0,0.0,2.0,...|\n",
      "|       0|[1.0,0.0,0.0,3.0,...|\n",
      "|       0|[3.0,3.0,1.0,0.0,...|\n",
      "|       1|[3.0,0.0,2.0,1.0,...|\n",
      "|       1|[2.0,1.0,0.0,0.0,...|\n",
      "|       1|[3.0,1.0,1.0,0.0,...|\n",
      "|       1|[1.0,0.0,0.0,3.0,...|\n",
      "|       0|[3.0,0.0,0.0,1.0,...|\n",
      "|       0|[3.0,1.0,5.0,2.0,...|\n",
      "|       0| (6,[0,5],[3.0,1.0])|\n",
      "|       1|[2.0,0.0,0.0,3.0,...|\n",
      "|       0|[3.0,4.0,1.0,0.0,...|\n",
      "|       0|[3.0,1.0,0.0,1.0,...|\n",
      "|       0|[2.0,0.0,0.0,2.0,...|\n",
      "|       1|[2.0,0.0,0.0,2.0,...|\n",
      "|       1|[3.0,0.0,0.0,0.0,...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['Pclass', 'SibSp', 'Parch', 'BucketedAge', 'BucketedFare', 'IndexedSex'], outputCol='features')\n",
    "train_cleaned = assembler.transform(train_cleaned)\n",
    "cols = ['Pclass', 'SibSp', 'Parch', 'BucketedAge', 'BucketedFare', 'IndexedSex']\n",
    "train_cleaned = train_cleaned.drop(*cols)\n",
    "train_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassificationModel (uid=RandomForestClassifier_414890e91c229ddf998d) with 10 trees"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='Survived', featuresCol='features', numTrees=10)\n",
    "rf.fit(train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='RandomForestClassifier_414890e91c229ddf998d', name='numTrees', doc='Number of trees to train (>= 1).')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def sexToInt(value):\n",
    "    if value == 'male':\n",
    "        return 0\n",
    "    elif value == 'female':\n",
    "        return 1\n",
    "\n",
    "def ageToInt(value):\n",
    "    if value <= 16:\n",
    "        return 0\n",
    "    elif value > 16 and value <= 32:\n",
    "        return 1\n",
    "    elif value > 32 and value <= 48:\n",
    "        return 2\n",
    "    elif value > 48 and value <= 64:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "def fareToInt(value):\n",
    "    if value <= 7.91:\n",
    "        return 0\n",
    "    elif value > 7.91 and value <= 14.454:\n",
    "        return 1\n",
    "    elif value > 14.454 and value <= 31:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "udfSexValueToInt = udf(sexToInt, IntegerType())\n",
    "udfAgeValueToInt = udf(ageToInt, IntegerType())\n",
    "udfFareValueToInt = udf(fareToInt, IntegerType())\n",
    "\n",
    "full_data_cleaned = []\n",
    "\n",
    "for dataset in [train, test]:\n",
    "    dataset_cleaned = dataset.withColumn('SexCategory', udfSexValueToInt('Sex'))\n",
    "    dataset_cleaned = dataset_cleaned.drop('Sex')\n",
    "    \n",
    "    dataset_cleaned = dataset_cleaned.withColumn('AgeCategory', udfAgeValueToInt('Age'))\n",
    "    dataset_cleaned = dataset_cleaned.drop('Age')\n",
    "    \n",
    "    dataset_cleaned = dataset_cleaned.withColumn('FareCategory', udfFareValueToInt('Fare'))\n",
    "    dataset_cleaned = dataset_cleaned.drop('Fare')\n",
    "    \n",
    "    full_data_cleaned.append(dataset_cleaned)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_data_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-cb2fbd3a1009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_data_cleaned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'full_data_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "full_data_cleaned[0].head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
